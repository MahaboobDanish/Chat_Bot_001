{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b946c423",
   "metadata": {},
   "source": [
    "# A simple chatBot with open source LLMs using Python and HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de65ea9",
   "metadata": {},
   "source": [
    "Transformers and LLMs work together within a chatbot to enable conversation.\n",
    "The interaction goes like\n",
    "1. Input Processing: message->chatbot, the transfoemer helps process the input, breaks down the message into smaller parts ('token') and reperesents in a way the chatbot understandss\n",
    "2. Understanding Context:Transformers passes these tokens to LLM, they understand with their abilities whcih were trained before.\n",
    "3. Generating Response: Tranformers takes the response generated from LLM and sent back to us in a understandable way.\n",
    "4. Iterative Conversation: The process repeats as the converstaion continues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bd493",
   "metadata": {},
   "source": [
    "# 1. Installing the Requirements\n",
    "'tansformers' is a open source Natural Language Processing(NLP) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3181d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588c22e",
   "metadata": {},
   "source": [
    "# 2. Import Autotokenizer and AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d066dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe79ef",
   "metadata": {},
   "source": [
    "# 3. Choose the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54abea",
   "metadata": {},
   "source": [
    "Varied LLMs differ form each other and use respectively. The different kinds are following\n",
    "a. Text Geeneration: a general purpose text generation can be done using GPT-2 or GPT-3 models\n",
    "\n",
    "b. Sentimental Analysis: Models like BERT or RoBERTa are popular as they are trained to understand the sentiment and emotional tone of text. (eg: positive/negative feedback of customer feedbacks).\n",
    "\n",
    "c. Named Entity Recognitions: LLMs such as BERT, GPT-2 or RoBERTa can be used for Named Entity Recognotion (NER) tasks. (eg: Extracting names and places from a given text.)\n",
    "\n",
    "d. Question Answering: BERT, GPT-2, XLNet can be effective for question and answering task. (eg: cahtbot that answer factual question from a given set of documents)\n",
    "\n",
    "e. Language Translation: Models like MarianMT or T5, used for translating text between different languages. (eg: Build a language Translation tool that translates English to French)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee63d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectionof Model\n",
    "model_name = \"facebook/blenderbot-400M-distill\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86217b",
   "metadata": {},
   "source": [
    "# 4. Fetch the Model and initialize a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb3b4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model: is an instance of the class AutoModelForSeq2SeqLM\n",
    "# Tokenizer: is an instance of the class AutoTokenizer \n",
    "# which optimizes our input and passes it to language models efficiency\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a157d",
   "metadata": {},
   "source": [
    "# 5. Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9a427",
   "metadata": {},
   "source": [
    "1. Initialize Model\n",
    "\n",
    "2. Encode conversation history as a string\n",
    "\n",
    "3. Fetch prompt from user\n",
    "\n",
    "4. Tokenize (optimize) prompt\n",
    "\n",
    "5. generate output from model usuing prompt and history\n",
    "\n",
    "6. Decode output\n",
    "\n",
    "7. Update conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9864353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation history>> [input1,input2,input3]\n",
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4311e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'transformers' library function we are using expects to \n",
    "# receive the conversation history as a string\n",
    "\n",
    "history_string = \"\\n\".join(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fd558ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"hello\"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ead36800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1710,   86]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization or Vectorization is the process of converting tokens into numerical represnetation.\n",
    "# In NLP oftenly 'encode_plus' is used from tokenizer.\n",
    "\n",
    "inputs = tokenizer.encode_plus(history_string, input_text,return_tensors = 'pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5471da2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': {'facebook/blenderbot-3B': 'https://huggingface.co/facebook/blenderbot-3B/resolve/main/vocab.json'},\n",
       " 'merges_file': {'facebook/blenderbot-3B': 'https://huggingface.co/facebook/blenderbot-3B/resolve/main/merges.txt'},\n",
       " 'tokenizer_config_file': {'facebook/blenderbot-3B': 'https://huggingface.co/facebook/blenderbot-3B/resolve/main/tokenizer_config.json'}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dpython dictionary which contains special keywords that allowed the model to properly\n",
    "# reference it contents. use 'pretrained_vocab_files_map' attribute to learn more about tokens\n",
    "\n",
    "tokenizer.pretrained_vocab_files_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3122c60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 1710, 1710, 1710, 6950, 6950, 6950, 1710, 1710, 7336, 7336, 7336,\n",
       "         4424, 7336, 7336, 1710, 1710, 4909, 1710, 1710, 3156, 1710, 1710,    2]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate outputs from Model\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e87687f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell hell hell Hello Hello Hello hell hell hi hi hi Hi hi hi hell hell hey hell hell Hell hell hell'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we got the outputs which are token they need to be decoded\n",
    "response = tokenizer.decode(outputs[0],skip_special_tokens=True).strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f4130d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hell hell hell Hello Hello Hello hell hell hi hi hi Hi hi hi hell hell hey hell hell Hell hell hell']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as the required output is generated, now this needs to be updated to conversation history\n",
    "conversation_history.append(input_text)\n",
    "conversation_history.append(response)\n",
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1f4f81c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hi\n",
      "Hi Hi Hi hi Hi Hi hell hell hell to hell hell I hi hi Hello Hello hi hi\n",
      "> How are You ?\n",
      "Hello hello hello how are you today? I am doing well. How are you?\n",
      "> I need help\n",
      "I'm doing well, thank you. What do you do for a living? I'm an accountant.\n",
      "> Bye\n",
      "I am a teacher. I love my job. What kind of music do you listen to?\n"
     ]
    }
   ],
   "source": [
    "# Now Repeat the process\n",
    "while input_text not in ['Bye']:\n",
    "# while True:\n",
    "    history_string = \"\\n\".join(conversation_history)\n",
    "    \n",
    "    #get input\n",
    "    input_text = input(\"> \")\n",
    "    \n",
    "    # tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(history_string,input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # generate the response\n",
    "    outputs = model.generate(**inputs,max_length = 60)\n",
    "    \n",
    "    # Decode the responses\n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True).strip()\n",
    "    \n",
    "    #Add interactio to conversation history\n",
    "    conversation_history.append(input_text)\n",
    "    conversation_history.append(response)\n",
    "    print(response)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f89e347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hell hell hell Hello Hello Hello hell hell hi hi hi Hi hi hi hell hell hey hell hell Hell hell hell']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37352cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
