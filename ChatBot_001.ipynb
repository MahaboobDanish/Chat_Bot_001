{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b946c423",
   "metadata": {},
   "source": [
    "# A simple chatBot with open source LLMs using Python and HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de65ea9",
   "metadata": {},
   "source": [
    "Transformers and LLMs work together within a chatbot to enable conversation.\n",
    "The interaction goes like\n",
    "1. Input Processing: message->chatbot, the transfoemer helps process the input, breaks down the message into smaller parts ('token') and reperesents in a way the chatbot understandss\n",
    "2. Understanding Context:Transformers passes these tokens to LLM, they understand with their abilities whcih were trained before.\n",
    "3. Generating Response: Tranformers takes the response generated from LLM and sent back to us in a understandable way.\n",
    "4. Iterative Conversation: The process repeats as the converstaion continues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bd493",
   "metadata": {},
   "source": [
    "# 1. Installing the Requirements\n",
    "'tansformers' is a open source Natural Language Processing(NLP) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3181d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588c22e",
   "metadata": {},
   "source": [
    "# 2. Import Autotokenizer and AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d066dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe79ef",
   "metadata": {},
   "source": [
    "# 3. Choose the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54abea",
   "metadata": {},
   "source": [
    "Varied LLMs differ form each other and use respectively. The different kinds are following\n",
    "a. Text Geeneration: a general purpose text generation can be done using GPT-2 or GPT-3 models\n",
    "\n",
    "b. Sentimental Analysis: Models like BERT or RoBERTa are popular as they are trained to understand the sentiment and emotional tone of text. (eg: positive/negative feedback of customer feedbacks).\n",
    "\n",
    "c. Named Entity Recognitions: LLMs such as BERT, GPT-2 or RoBERTa can be used for Named Entity Recognotion (NER) tasks. (eg: Extracting names and places from a given text.)\n",
    "\n",
    "d. Question Answering: BERT, GPT-2, XLNet can be effective for question and answering task. (eg: cahtbot that answer factual question from a given set of documents)\n",
    "\n",
    "e. Language Translation: Models like MarianMT or T5, used for translating text between different languages. (eg: Build a language Translation tool that translates English to French)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee63d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectionof Model\n",
    "model_name = \"facebook/blenderbot-400M-distill\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86217b",
   "metadata": {},
   "source": [
    "# 4. Fetch the Model and initialize a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3b4b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danishkarur/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/danishkarur/opt/anaconda3/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/danishkarur/opt/anaconda3/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/danishkarur/opt/anaconda3/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model: is an instance of the class AutoModelForSeq2SeqLM\n",
    "# Tokenizer: is an instance of the class AutoTokenizer \n",
    "# which optimizes our input and passes it to language models efficiency\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a157d",
   "metadata": {},
   "source": [
    "# 5. Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9a427",
   "metadata": {},
   "source": [
    "1. Initialize Model\n",
    "\n",
    "2. Encode conversation history as a string\n",
    "\n",
    "3. Fetch prompt from user\n",
    "\n",
    "4. Tokenize (optimize) prompt\n",
    "\n",
    "5. generate output from model usuing prompt and history\n",
    "\n",
    "6. Decode output\n",
    "\n",
    "7. Update conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9864353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation history>> [input1,input2,input3]\n",
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4311e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'transformers' library function we are using expects to \n",
    "# receive the conversation history as a string\n",
    "\n",
    "history_string = \"\\n\".join(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd558ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"hello\"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead36800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1710,   86]]), 'attention_mask': tensor([[1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization or Vectorization is the process of converting tokens into numerical represnetation.\n",
    "# In NLP oftenly 'encode_plus' is used from tokenizer.\n",
    "\n",
    "inputs = tokenizer.encode_plus(history_string, input_text,return_tensors = 'pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5471da2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dpython dictionary which contains special keywords that allowed the model to properly\n",
    "# reference it contents. use 'pretrained_vocab_files_map' attribute to learn more about tokens\n",
    "\n",
    "tokenizer.pretrained_vocab_files_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3122c60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 1710, 1710, 1710, 6950, 6950, 6950, 1710, 1710, 7336, 7336, 7336,\n",
       "         4424, 7336, 7336, 1710, 1710, 4909, 1710, 1710, 3156, 1710, 1710,    2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate outputs from Model\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e87687f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell hell hell Hello Hello Hello hell hell hi hi hi Hi hi hi hell hell hey hell hell Hell hell hell'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we got the outputs which are token they need to be decoded\n",
    "response = tokenizer.decode(outputs[0],skip_special_tokens=True).strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4130d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hell hell hell Hello Hello Hello hell hell hi hi hi Hi hi hi hell hell hey hell hell Hell hell hell']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as the required output is generated, now this needs to be updated to conversation history\n",
    "conversation_history.append(input_text)\n",
    "conversation_history.append(response)\n",
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f81c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now Repeat the process\n",
    "while input_text not in ['Bye']:\n",
    "# while True:\n",
    "    history_string = \"\\n\".join(conversation_history)\n",
    "    \n",
    "    #get input\n",
    "    input_text = input(\"> \")\n",
    "    \n",
    "    # tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(history_string,input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # generate the response\n",
    "    outputs = model.generate(**inputs,max_length = 60)\n",
    "    \n",
    "    # Decode the responses\n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True).strip()\n",
    "    \n",
    "    #Add interactio to conversation history\n",
    "    conversation_history.append(input_text)\n",
    "    conversation_history.append(response)\n",
    "    print(response)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37352cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
